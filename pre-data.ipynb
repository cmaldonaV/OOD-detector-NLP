{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20NG, R52, R8\n",
    "@Misc{2007:phd-Ana-Cardoso-Cachopo,\n",
    "\n",
    "  author = {Ana Cardoso-Cachopo},\n",
    "\n",
    "  title = {{Improving Methods for Single-label Text Categorization}},\n",
    "\n",
    "  howpublished = {PdD Thesis, Instituto Superior Tecnico, Universidade Tecnica de Lisboa},\n",
    "\n",
    "  year = 2007} \n",
    "  \n",
    "  https://ana.cachopo.org/datasets-for-single-label-text-categorization\n",
    "  \n",
    "  **Pre-processing\n",
    "Except for the Cade12 dataset, from the original datasets, in order to obtain the present files, I applied the following pre-processing:**\n",
    "\n",
    "* Substitute TAB, NEWLINE and RETURN characters by SPACE.\n",
    "\n",
    "* Keep only letters (that is, turn punctuation, numbers, etc. into SPACES).\n",
    "\n",
    "* Turn all letters to lowercase.\n",
    "\n",
    "* Substitute multiple SPACES by a single SPACE.\n",
    "\n",
    "* The title/subject of each document is simply added in the beginning of the document's text.\n",
    "\n",
    "* no-short Obtained from the previous file, by removing words that are less than 3 characters long. For example, removing \"he\" but keeping \"him\".\n",
    "\n",
    "* no-stop Obtained from the previous file, by removing the 524 SMART stopwords. Some of them had already been removed, because they were shorter than 3 characters.\n",
    "\n",
    "* stemmed Obtained from the previous file, by applying Porter's Stemmer to the remaining words. Information about stemming can be found here.\n",
    "  \n",
    "\n",
    "# CLINC150\n",
    "\n",
    "from their github\n",
    "https://github.com/clinc/oos-eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from __future__ import print_function\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy import array, asarray, zeros\n",
    "from scipy import stats\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import string\n",
    "\n",
    "# Seeds\n",
    "np.random.seed(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Camilo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Camilo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing text\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = stopwords.words('english')\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def prepro_text(text):\n",
    "    text = text.lower()\n",
    "    text_p = \"\".join([char for char in text if char not in string.punctuation]) \n",
    "    words = nltk.word_tokenize(text_p)\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    stemmed = [porter.stem(word) for word in filtered_words]\n",
    "    return ' '.join(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R52 DATASET\n",
      "                                               text_raw label  group  \\\n",
      "5128  borg warner up amid rumors irwin jacobs sold s...   acq  train   \n",
      "4171  ambrit inc abi th qtr jan net shr cts vs nil n...  earn  train   \n",
      "2765  japan foreign shipbuilding orders rise in febr...  ship  train   \n",
      "3632  group has pct of atlantic research atrc a grou...   acq  train   \n",
      "6210  agency reports ships waiting at panama canal t...  ship  train   \n",
      "\n",
      "                                                   text  class  \n",
      "5128  borg warner amid rumor irwin jacob sold stock ...      0  \n",
      "4171  ambrit inc abi th qtr jan net shr ct vs nil ne...     12  \n",
      "2765  japan foreign shipbuild order rise februari ne...     43  \n",
      "3632  group pct atlant research atrc group led halcy...      0  \n",
      "6210  agenc report ship wait panama canal panama can...     43  \n",
      "------------------------------------------------\n",
      "20NG DATASET\n",
      "                                                text_raw  \\\n",
      "2460   re can i get more than x on monitor in article...   \n",
      "9896   re wtc bombing tclock orion oac uci edu tim cl...   \n",
      "15591  re transparent cyphertext aej d cmuvm bitnet w...   \n",
      "12978  re monitors should they be kept on hours a day...   \n",
      "5646   re hispanic all star team icop csa bu edu anto...   \n",
      "\n",
      "                       label group  \\\n",
      "2460   comp.sys.mac.hardware  test   \n",
      "9896   talk.politics.mideast  test   \n",
      "15591              sci.crypt  test   \n",
      "12978  comp.sys.mac.hardware  test   \n",
      "5646      rec.sport.baseball  test   \n",
      "\n",
      "                                                    text  class  \n",
      "2460   get x monitor articl carsona sfu ca carsona fr...      4  \n",
      "9896   wtc bomb tclock orion oac uci edu tim clock wr...     17  \n",
      "15591  transpar cyphertext aej cmuvm bitnet wrote lig...     11  \n",
      "12978  monitor kept hour day cnn factiod last month s...      4  \n",
      "5646   hispan star team icop csa bu edu antonio pera ...      9  \n",
      "------------------------------------------------\n",
      "CLINC150 DATASET\n",
      "                                                text_raw               label  \\\n",
      "10293       what ingredients do i need to make spaghetti    ingredients_list   \n",
      "22243  i want to cancel my reservation for network to...  cancel_reservation   \n",
      "9471                           show me where my order is        order_status   \n",
      "3133                               how is my name listed           user_name   \n",
      "7234                                      call my mother           make_call   \n",
      "\n",
      "       group                             text  class  \n",
      "10293  train      ingredi need make spaghetti     56  \n",
      "22243    val  want cancel reserv network xaga     16  \n",
      "9471   train                       show order     82  \n",
      "3133   train                        name list    137  \n",
      "7234   train                      call mother     65  \n",
      "------------------------------------------------\n",
      "TREC DATASET\n",
      "                                            text_raw label  group  \\\n",
      "0  manner How did serfdom develop in and then lea...  DESC  train   \n",
      "1  cremat What films featured the character Popey...  ENTY  train   \n",
      "2  manner How can I find a list of celebrities ' ...  DESC  train   \n",
      "3  animal What fowl grabs the spotlight after the...  ENTY  train   \n",
      "4                  exp What is the full form of .com  ABBR  train   \n",
      "\n",
      "                                          text  class  \n",
      "0           manner serfdom develop leav russia      1  \n",
      "1        cremat film featur charact popey doyl      2  \n",
      "2            manner find list celebr real name      1  \n",
      "3  anim fowl grab spotlight chines year monkey      2  \n",
      "4                            exp full form com      0  \n"
     ]
    }
   ],
   "source": [
    "path_data = '\\\\Users\\\\Camilo\\\\Desktop\\\\tesis\\\\codes\\\\data\\\\'\n",
    "# ------------------  r52 DATASET -----------------------\n",
    "\n",
    "# Train-Validation examples\n",
    "print('R52 DATASET')\n",
    "X = list()\n",
    "y = list()\n",
    "group = list()\n",
    "\n",
    "# Train data\n",
    "f = open(path_data+'r52-train-all-terms.txt')\n",
    "data = list()\n",
    "for line in f:\n",
    "    X.append(' '.join(line.split()[1:]))\n",
    "    y.append(line.split()[0])\n",
    "    group.append('train')\n",
    "f.close()\n",
    "\n",
    "# Test data\n",
    "f = open(path_data+'r52-test-all-terms.txt')\n",
    "labels = set()\n",
    "for line in f:\n",
    "    X.append(' '.join(line.split()[1:]))\n",
    "    y.append(line.split()[0])\n",
    "    group.append('test')\n",
    "f.close()\n",
    "\n",
    "# Create DataFrame\n",
    "df_r = pd.DataFrame(list(zip(X, y)), \n",
    "              columns =['text_raw', 'label']) \n",
    "df_r['group'] = group\n",
    "df_r['text'] = df_r['text_raw'].apply(lambda x: prepro_text(x))\n",
    "#df_r['large'] = df_r['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Label to number class\n",
    "le = LabelEncoder()\n",
    "le.fit(df_r['label'].unique())\n",
    "df_r['class'] = df_r['label'].apply(lambda x: le.transform([x])[0])\n",
    "np.save(path_data+'r52-classesEncoder.npy', le.classes_)\n",
    "\n",
    "print(df_r.sample(5))\n",
    "print('------------------------------------------------')\n",
    "\n",
    "# ------------------  20NG DATASET -----------------------\n",
    "print('20NG DATASET')\n",
    "X = list()\n",
    "y = list()\n",
    "group = list()\n",
    "\n",
    "# Train data\n",
    "f = open(path_data+'20ng-train-all-terms.txt')\n",
    "data = list()\n",
    "for line in f:\n",
    "    X.append(' '.join(line.split()[1:]))\n",
    "    y.append(line.split()[0])\n",
    "    group.append('test')\n",
    "f.close()\n",
    "\n",
    "# Train-Val\n",
    "for lab, text in data:\n",
    "    X.append(' '.join(text))\n",
    "    y.append(lab)\n",
    "    group.append('train')\n",
    "\n",
    "# Test data\n",
    "f = open(path_data+'20ng-test-all-terms.txt')\n",
    "for line in f:\n",
    "    X.append(' '.join(line.split()[1:]))\n",
    "    y.append(line.split()[0])\n",
    "    group.append('test')\n",
    "f.close()\n",
    "\n",
    "# Create DataFrame\n",
    "df_ng = pd.DataFrame(list(zip(X, y)), \n",
    "              columns =['text_raw', 'label']) \n",
    "df_ng['group'] = group\n",
    "df_ng['text'] = df_ng['text_raw'].apply(lambda x: prepro_text(x))\n",
    "#df_ng['large'] = df_ng['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Label to number class\n",
    "le = LabelEncoder()\n",
    "le.fit(df_ng['label'].unique())\n",
    "df_ng['class'] = df_ng['label'].apply(lambda x: le.transform([x])[0])\n",
    "np.save(path_data+'20ng-classesEncoder.npy', le.classes_)\n",
    "    \n",
    "print(df_ng.sample(5))\n",
    "print('------------------------------------------------')\n",
    "\n",
    "# ------------------  CLINC150 DATASET -----------------------\n",
    "print('CLINC150 DATASET')\n",
    "X = list()\n",
    "y = list()\n",
    "group = list()\n",
    "f = open(path_data+'clinc150_data_full.json')\n",
    "data = json.load(f) \n",
    "data.keys()\n",
    "# Train\n",
    "for text,label in data['train']:\n",
    "    X.append(text)\n",
    "    y.append(label)\n",
    "    group.append('train')\n",
    "# Test\n",
    "for text,label in data['test']: \n",
    "    X.append(text)\n",
    "    y.append(label)\n",
    "    group.append('test')\n",
    "# Validation\n",
    "for text,label in data['val']: \n",
    "    X.append(text)\n",
    "    y.append(label)    \n",
    "    group.append('val')\n",
    "f.close()\n",
    "\n",
    "df_clinc = pd.DataFrame(list(zip(X, y)), \n",
    "              columns =['text_raw', 'label']) \n",
    "df_clinc['group'] = group\n",
    "df_clinc['text'] = df_clinc['text_raw'].apply(lambda x: prepro_text(x))\n",
    "#df_clinc['large'] = df_clinc['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Label to number class\n",
    "le = LabelEncoder()\n",
    "le.fit(df_clinc['label'].unique())\n",
    "df_clinc['class'] = df_clinc['label'].apply(lambda x: le.transform([x])[0])\n",
    "np.save(path_data+'clinc150-classesEncoder.npy', le.classes_)\n",
    "\n",
    "print(df_clinc.sample(5))\n",
    "print('------------------------------------------------')\n",
    "\n",
    "# ------------------  TREC DATASET -----------------------\n",
    "print('TREC DATASET')\n",
    "\n",
    "X = list()\n",
    "y = list()\n",
    "group = list()\n",
    "\n",
    "f = open(path_data +'trec6-train.txt')\n",
    "for line in f:\n",
    "    line = line.replace('?','')\n",
    "    X.append(line.split(':')[1].strip())\n",
    "    y.append(line.split(':')[0])\n",
    "    group.append('train')\n",
    "f.close()\n",
    "\n",
    "f = open(path_data +'trec6-test.txt')\n",
    "for line in f:\n",
    "    line = line.replace('?','')\n",
    "    X.append(line.split(':')[1].strip())\n",
    "    y.append(line.split(':')[0])\n",
    "    group.append('test')\n",
    "f.close()\n",
    "\n",
    "df_trec = pd.DataFrame(list(zip(X, y)), \n",
    "              columns =['text_raw', 'label']) \n",
    "df_trec['group'] = group\n",
    "df_trec['text'] = df_trec['text_raw'].apply(lambda x: prepro_text(x))\n",
    "#df_trec['large'] = df_trec['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Label to number class\n",
    "le = LabelEncoder()\n",
    "le.fit(df_trec['label'].unique())\n",
    "df_trec['class'] = df_trec['label'].apply(lambda x: le.transform([x])[0])\n",
    "np.save(path_data+'trec6-classesEncoder.npy', le.classes_)\n",
    "\n",
    "print(df_trec.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create validation split in train set\n",
    "\n",
    "Datasets:\n",
    " * 20NG\n",
    " * R52\n",
    " * TREC6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_val(df,p=0.2):\n",
    "    frames = list()\n",
    "    labels = df['class'].unique()\n",
    "    for c in labels:\n",
    "        df_aux = df[df['class'] == c].copy()\n",
    "        arr = df_aux.group.values\n",
    "        l = len(df_aux[df_aux.group == 'train']['group'])*p\n",
    "        c = 0\n",
    "        ind = np.random.randint(0,len(arr))\n",
    "        while (c < l):\n",
    "            if (arr[ind] == 'train'):\n",
    "                arr[ind] = 'val'\n",
    "                c += 1\n",
    "            ind = np.random.randint(0,len(arr))\n",
    "        df_aux['group'] = arr\n",
    "        frames.append(df_aux)\n",
    "    return pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_val(df_trec) \n",
    "df.to_csv(path_or_buf=path_data+'trecData.csv', sep='@',index=False)\n",
    "\n",
    "df = create_val(df_clinc) \n",
    "df.to_csv(path_or_buf=path_data+'clincData.csv', sep='@',index=False)\n",
    "\n",
    "df = create_val(df_ng) \n",
    "df.to_csv(path_or_buf=path_data+'ngData.csv', sep='@',index=False)\n",
    "\n",
    "df = create_val(df_r) \n",
    "df.to_csv(path_or_buf=path_data+'rData.csv', sep='@',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv(path_data+'trecData.csv', delimiter='@')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder = LabelEncoder()\n",
    "#encoder.classes_ = np.load(path_data+'classes.npy',allow_pickle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
